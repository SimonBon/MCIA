{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tifffile import imread\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base = Path('/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/data/cHL_data/zenodo_files/cHL_1_MIBI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list((base / 'raw_image').glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/32:  30%|██▉       | 14/47 [00:06<00:15,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/32: 100%|██████████| 47/47 [00:21<00:00,  2.22it/s]\n",
      "1/32:  30%|██▉       | 14/47 [00:10<00:24,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/32: 100%|██████████| 47/47 [00:34<00:00,  1.35it/s]\n",
      "2/32:  36%|███▌      | 17/47 [00:01<00:02, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/32: 100%|██████████| 47/47 [00:04<00:00, 10.74it/s]\n",
      "3/32:  30%|██▉       | 14/47 [00:03<00:07,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/32: 100%|██████████| 47/47 [00:11<00:00,  4.24it/s]\n",
      "4/32:  34%|███▍      | 16/47 [00:02<00:03,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/32: 100%|██████████| 47/47 [00:07<00:00,  6.50it/s]\n",
      "5/32:  36%|███▌      | 17/47 [00:01<00:02, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/32: 100%|██████████| 47/47 [00:04<00:00, 11.23it/s]\n",
      "6/32:  30%|██▉       | 14/47 [00:06<00:13,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/32: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s]\n",
      "7/32:  30%|██▉       | 14/47 [00:06<00:15,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/32: 100%|██████████| 47/47 [00:22<00:00,  2.09it/s]\n",
      "8/32:  30%|██▉       | 14/47 [00:06<00:18,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/32: 100%|██████████| 47/47 [00:20<00:00,  2.35it/s]\n",
      "9/32:  30%|██▉       | 14/47 [00:03<00:07,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/32: 100%|██████████| 47/47 [00:16<00:00,  2.86it/s]\n",
      "10/32:  30%|██▉       | 14/47 [00:06<00:17,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/32: 100%|██████████| 47/47 [00:23<00:00,  1.98it/s]\n",
      "11/32:  30%|██▉       | 14/47 [00:06<00:14,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/32: 100%|██████████| 47/47 [00:26<00:00,  1.78it/s]\n",
      "12/32:  30%|██▉       | 14/47 [00:14<00:40,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/32: 100%|██████████| 47/47 [00:50<00:00,  1.08s/it]\n",
      "13/32:  30%|██▉       | 14/47 [00:04<00:07,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13/32: 100%|██████████| 47/47 [00:12<00:00,  3.89it/s]\n",
      "14/32:  30%|██▉       | 14/47 [00:06<00:15,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14/32: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s]\n",
      "15/32:  30%|██▉       | 14/47 [00:13<00:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15/32: 100%|██████████| 47/47 [00:47<00:00,  1.02s/it]\n",
      "16/32:  30%|██▉       | 14/47 [00:08<00:25,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/32: 100%|██████████| 47/47 [00:29<00:00,  1.62it/s]\n",
      "17/32:  30%|██▉       | 14/47 [00:14<00:43,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/32: 100%|██████████| 47/47 [00:50<00:00,  1.07s/it]\n",
      "18/32:  30%|██▉       | 14/47 [00:07<00:21,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/32: 100%|██████████| 47/47 [00:24<00:00,  1.90it/s]\n",
      "19/32:  30%|██▉       | 14/47 [00:04<00:09,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/32: 100%|██████████| 47/47 [00:17<00:00,  2.69it/s]\n",
      "20/32:  30%|██▉       | 14/47 [00:08<00:19,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/32: 100%|██████████| 47/47 [00:26<00:00,  1.77it/s]\n",
      "21/32:  30%|██▉       | 14/47 [00:08<00:14,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/32: 100%|██████████| 47/47 [00:28<00:00,  1.63it/s]\n",
      "22/32:  30%|██▉       | 14/47 [00:07<00:18,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/32: 100%|██████████| 47/47 [00:23<00:00,  1.97it/s]\n",
      "23/32:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/32: 100%|██████████| 47/47 [00:05<00:00,  8.35it/s]\n",
      "24/32:  30%|██▉       | 14/47 [00:07<00:17,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/32: 100%|██████████| 47/47 [00:20<00:00,  2.27it/s]\n",
      "25/32:  30%|██▉       | 14/47 [00:05<00:14,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/32: 100%|██████████| 47/47 [00:17<00:00,  2.67it/s]\n",
      "26/32:  30%|██▉       | 14/47 [00:12<00:32,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/32: 100%|██████████| 47/47 [00:36<00:00,  1.29it/s]\n",
      "27/32:  30%|██▉       | 14/47 [00:07<00:16,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27/32: 100%|██████████| 47/47 [00:26<00:00,  1.77it/s]\n",
      "28/32:  30%|██▉       | 14/47 [00:05<00:11,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28/32: 100%|██████████| 47/47 [00:20<00:00,  2.31it/s]\n",
      "29/32:  30%|██▉       | 14/47 [00:07<00:15,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29/32: 100%|██████████| 47/47 [00:27<00:00,  1.70it/s]\n",
      "30/32:  30%|██▉       | 14/47 [00:09<00:19,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30/32: 100%|██████████| 47/47 [00:29<00:00,  1.61it/s]\n",
      "31/32:  30%|██▉       | 14/47 [00:09<00:19,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31/32: 100%|██████████| 47/47 [00:26<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_dict = dict()\n",
    "for i, sample in enumerate(samples):\n",
    "    #print(sample)\n",
    "    sample_id = sample.stem\n",
    "    files = list(sample.glob('*'))\n",
    "        \n",
    "    #files = [Path(str(f).replace(' ', '')) for f in sample.glob('*')]\n",
    "    marker_image_dict = dict()\n",
    "    pbar = tqdm(files, desc=f\"{i}/{len(samples)}\")\n",
    "    for f in pbar:\n",
    "        marker = str(f.stem).replace(' ', '')\n",
    "        if marker == 'Segmentation':\n",
    "            print('Skipping Segmentation')\n",
    "            continue\n",
    "        #print(marker)\n",
    "        image = imread(f).T\n",
    "        #print(image.shape)\n",
    "        marker_image_dict[marker] = image\n",
    "        \n",
    "    segmentation = imread(base / f'segmentation/{sample_id}/H3_memSUM_noCD163_deepcell060_AutoHist_mpp1.75/segmentationMap.tif').T\n",
    "    marker_image_dict['segmentation'] = segmentation\n",
    "        \n",
    "    sample_dict[sample_id] = marker_image_dict\n",
    "        \n",
    "    if i == 5:\n",
    "        continue\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv(base / 'annotation_csv/cHL1_MIBI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M2', 'M2', 'M2', ..., 'Neutrophil', 'Neutrophil', 'Neutrophil'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_labels = annotations_df.Annotation.to_numpy()\n",
    "cell_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = annotations_df.centroidX.to_numpy()\n",
    "Y = annotations_df.centroidY.to_numpy()\n",
    "ID = annotations_df.identifier.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1669853, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identifier = np.stack((X, Y, ID), axis=1)\n",
    "identifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1669853, 3), (1669853,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids = [int(s) for s in sample_dict.keys()]\n",
    "mask = np.isin(identifier[:, 2], sample_ids)\n",
    "_id = identifier[mask]\n",
    "_cell_labels = cell_labels[mask]\n",
    "_id.shape, _cell_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3584, 3584, 46) (3584, 3584)\n",
      "(4608, 4608, 46) (4608, 4608)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(4096, 4096, 46) (4096, 4096)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(4096, 4096, 46) (4096, 4096)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(4096, 4096, 46) (4096, 4096)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(1024, 1024, 46) (1024, 1024)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(1536, 1536, 46) (1536, 1536)\n",
      "(3072, 3072, 46) (3072, 3072)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2048, 2048, 46) (2048, 2048)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2560, 2560, 46) (2560, 2560)\n",
      "(2560, 2560, 46) (2560, 2560)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "out_path = \"/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/dataset/cHL_MIBI.h5\"\n",
    "\n",
    "with h5py.File(out_path, \"w\") as f:\n",
    "    marker_list = None\n",
    "\n",
    "    for sample, images_dict in sample_dict.items():\n",
    "        if marker_list is None:\n",
    "            marker_list = [m for m in images_dict.keys() if m != \"segmentation\"]\n",
    "            marker_list = list(marker_list)\n",
    "\n",
    "            str_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "            f.create_dataset(\n",
    "                name=\"MARKERS\",\n",
    "                data=np.array(marker_list, dtype=str_dtype)\n",
    "            )\n",
    "            \n",
    "        images = np.array([images_dict[m] for m in marker_list]).transpose(1,2,0)\n",
    "        masks = np.array(images_dict['segmentation'])\n",
    "        print(images.shape, masks.shape)\n",
    "\n",
    "        sample = f.create_group(name=str(sample))\n",
    "        sample.create_dataset(name='IMAGES', data=images, chunks=(64, 64, 1), compression=3)\n",
    "        sample.create_dataset(name='MASKS', data=masks, chunks=(64, 64))\n",
    "        \n",
    "    f.create_dataset(name='INDEXER', data=_id)\n",
    "    f.create_dataset(name='IMAGE_KEYS', data=np.array(list(sample_dict.keys()), dtype=str_dtype))\n",
    "    f.create_dataset(name='ANNOTATIONS', data=_cell_labels, dtype=str_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon_g/anaconda3/envs/MIDL26/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "from mmengine.registry import DATASETS\n",
    "\n",
    "# @DATASETS.register_module()\n",
    "class MultiChannelDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                h5_file_path,\n",
    "                patch_size,\n",
    "                shuffle: bool = False,\n",
    "                markers_to_use= None,\n",
    "                in_memory=False,\n",
    "                marker_key='MARKERS',\n",
    "                pipeline=None,\n",
    "                additional_keys=[],\n",
    "                split=[0.7, 0.1, 0.2],\n",
    "                used_split='training',\n",
    "                mask_image=False, \n",
    "                classes_to_ignore=None,\n",
    "                **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.random_state = kwargs.get('random_state', 42)\n",
    "        np.random.seed(self.random_state)\n",
    "           \n",
    "        # Ensure the HDF5 file exists\n",
    "        assert Path(h5_file_path).exists(), f\"Provided path to h5 file does not exist: {h5_file_path}\"\n",
    "        \n",
    "        self.marker_key = marker_key\n",
    "        self.additional_keys = additional_keys\n",
    "        self.split = split\n",
    "        self.used_split = used_split\n",
    "        self.mask_image = mask_image\n",
    "        self.classes_to_ignore = classes_to_ignore\n",
    "        self.markers_to_use = markers_to_use\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "        self.indexer = self.h5_file['INDEXER'][:]\n",
    "        annotations = self.h5_file['ANNOTATIONS'][()].astype(str) if 'ANNOTATIONS' in self.h5_file.keys() else None\n",
    "        \n",
    "        self.annotations = annotations\n",
    "        if self.classes_to_ignore is not None and annotations is not None:\n",
    "            idxs = set(list(range(len(self.indexer))))\n",
    "            ignored_idxs = []\n",
    "            for ignored_class in self.classes_to_ignore:\n",
    "                class_idxs = np.where(self.annotations == ignored_class)[0]\n",
    "                ignored_idxs.extend(class_idxs)\n",
    "                print(f'Removing {len(class_idxs)} instances of class:\"{ignored_class}\"')\n",
    "            self.row_indexer = np.array(list(idxs - set(ignored_idxs)))\n",
    "        else:\n",
    "            self.row_indexer = np.arange(len(self.indexer))\n",
    "            \n",
    "        if self.annotations is not None: \n",
    "            print(f'Dataset contains annotations')\n",
    "            \n",
    "        print(f'Dataset contains {len(self.indexer)} cells.')\n",
    "        \n",
    "        image_keys = self.h5_file['IMAGE_KEYS'][()].astype(str)\n",
    "        \n",
    "        if in_memory:\n",
    "        \n",
    "            print('Start loading images to memory')\n",
    "        \n",
    "            self.image_dict = dict()\n",
    "            for key in tqdm(image_keys):\n",
    "                self.image_dict[key] = self.h5_file[key][()]\n",
    "                \n",
    "            # if 'MASKS' in self.h5_file.keys():\n",
    "            #     self.mask_data = self.h5_file['MASKS'][:]\n",
    "            # else:\n",
    "            self.mask_data = None\n",
    "            \n",
    "            print('Finished loading images to memory')\n",
    "        else:\n",
    "            \n",
    "            self.image_dict = dict()\n",
    "            for key in image_keys:\n",
    "                self.image_dict[key] = self.h5_file[key]\n",
    "                \n",
    "            # if 'MASKS' in self.h5_file.keys():\n",
    "            #     self.mask_data = self.h5_file['MASKS'][:]\n",
    "            # else:\n",
    "            self.mask_data = None\n",
    "                            \n",
    "        self.marker_names = self.h5_file[marker_key][:].astype(str)\n",
    "        print(f\"All Markers: {self.marker_names}\")\n",
    "        \n",
    "        # Set patch size and calculate half patch size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch_size = self.hpsz = self.patch_size // 2\n",
    "        assert self.patch_size % 2 == 0, \"patch_size needs to be divisible by 2\"\n",
    "                \n",
    "        # self.channels_to_skip = channels_to_skip\n",
    "        if markers_to_use is not None:\n",
    "            self.markers_to_use = markers_to_use\n",
    "            \n",
    "            channel_idxs = []\n",
    "            for marker in self.markers_to_use:\n",
    "                if marker not in self.marker_names:\n",
    "                    raise ValueError(f'{marker} is not in marker names list!')\n",
    "                channel_idxs.append(np.where(self.marker_names == marker)[0][0])\n",
    "            self.marker_names = self.marker_names[channel_idxs]\n",
    "            self.used_channels = np.array(channel_idxs)\n",
    "            \n",
    "        else:\n",
    "            self.used_channels = np.arange(len(self.marker_names))\n",
    "            \n",
    "        print(f\"Used Markers: {self.marker_names}\")\n",
    "            \n",
    "        self.channel2idx = {channel: i for i, channel in enumerate(self.marker_names)}\n",
    "        self.idx2channel = {i: channel for i, channel in enumerate(self.marker_names)} \n",
    "                             \n",
    "        if self.shuffle:\n",
    "            print('shuffling dataset')\n",
    "            np.random.shuffle(self.row_indexer)\n",
    "\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = Compose(pipeline)\n",
    "        else:\n",
    "            self.pipeline = lambda x: x\n",
    "            \n",
    "        if self.split is not None:\n",
    "            \n",
    "            if used_split == 'all':\n",
    "                self.row_indexer = self.row_indexer\n",
    "                print(f'Using {self.used_split} split!')\n",
    "\n",
    "            else:\n",
    "            \n",
    "                assert len(self.split) == 3, f'Please provide a split of percentages for train, val, test'\n",
    "                assert sum(self.split) == 1, f'Please provide the splits as percentages that sum to 1'\n",
    "                \n",
    "                np.random.shuffle(self.row_indexer)\n",
    "            \n",
    "                n_train = int(len(self.row_indexer) * self.split[0])\n",
    "                n_val = int(len(self.row_indexer) * self.split[1])\n",
    "                n_test = len(self.row_indexer) - n_train - n_val\n",
    "                \n",
    "                print(n_train, n_val, n_test)\n",
    "                \n",
    "                train_idxs = np.sort(self.row_indexer[:n_train])\n",
    "                val_idxs = np.sort(self.row_indexer[n_train:(n_train+n_val)])\n",
    "                test_idxs = np.sort(self.row_indexer[(n_train+n_val):])\n",
    "                \n",
    "                if used_split == 'training':\n",
    "                    self.row_indexer = train_idxs\n",
    "                elif used_split == 'validation':\n",
    "                    self.row_indexer = val_idxs\n",
    "                elif used_split == 'test':\n",
    "                    self.row_indexer = test_idxs\n",
    "                    \n",
    "                print(f'Using {self.used_split} split!')\n",
    "            \n",
    "        #print(self.__repr__())\n",
    "            \n",
    "        # self.row_idxs = self.get_row_idxs()\n",
    "\n",
    "        # self.shuffle = shuffle\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.row_idxs)\n",
    "        #     print(\"Shuffling data order\")\n",
    "\n",
    "        # if pipeline is not None:\n",
    "        #     self.pipeline = Compose(pipeline)\n",
    "        # else:\n",
    "        #     self.pipeline = lambda x: x\n",
    "         \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.row_indexer)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx, eval=False, ommit_pipeline=False):\n",
    "        \n",
    "        dataset_idx = idx\n",
    "        idx = self.row_indexer[idx]\n",
    "        \n",
    "        x, y, sample = self.indexer[idx].astype(int)\n",
    "        curr_patch = self.get_patch(self.image_dict, sample, y, x, self.hpsz)[..., self.used_channels]\n",
    "        if self.mask_data is not None:\n",
    "            curr_mask = self.get_patch(self.mask_data, sample, y, x, self.hpsz)\n",
    "        else:\n",
    "            curr_mask = []\n",
    "            \n",
    "        if self.mask_image:\n",
    "            mask_value = curr_mask[curr_mask.shape[0]//2, curr_mask.shape[1]//2]\n",
    "            curr_mask[curr_mask != mask_value] = 0\n",
    "            curr_mask[curr_mask == mask_value] = 1\n",
    "            curr_patch = curr_patch * curr_mask[..., None]\n",
    "        \n",
    "        pipeline_dict = {\n",
    "            'img': curr_patch.astype(np.float32),\n",
    "            'batch': sample,\n",
    "            'masks': curr_mask,\n",
    "            'idx': idx,\n",
    "            'dataset_idx': dataset_idx,\n",
    "            '(x,y)': (x,y),\n",
    "            'annotation': self.annotations[idx] if self.annotations is not None else 'None'\n",
    "        }\n",
    "        \n",
    "\n",
    "        for key in self.additional_keys:\n",
    "            data = self.h5_file[key][idx]\n",
    "            data = data if type(data) != bytes else data.decode(\"utf-8\")\n",
    "            pipeline_dict[key] = data\n",
    "        \n",
    "        return  self.pipeline(pipeline_dict)\n",
    "    \n",
    "\n",
    "    def get_patch(self, image_dict, im_idx, y, x, hpsz):\n",
    "\n",
    "        curr_H, curr_W, _ = image_dict[str(im_idx)].shape\n",
    "        \n",
    "        x_min, x_max = max(x - hpsz, 0), min(x + hpsz, curr_H)\n",
    "        pad_x_min = max(0, hpsz - x)\n",
    "        pad_x_max = max(0, (x + hpsz) - curr_H)\n",
    "\n",
    "        y_min, y_max = max(y - hpsz, 0), min(y + hpsz, curr_W)\n",
    "        pad_y_min = max(0, hpsz - y)\n",
    "        pad_y_max = max(0, (y + hpsz) - curr_W)\n",
    "        \n",
    "        if pad_x_min + pad_x_max + pad_y_min + pad_y_max == 0:\n",
    "            img = image_dict[str(im_idx)][y_min:y_max, x_min:x_max]\n",
    "        else:\n",
    "            img = image_dict[str(im_idx)][y_min:y_max, x_min:x_max]\n",
    "            if img.ndim == 2:\n",
    "                padding = ((pad_y_min, pad_y_max), (pad_x_min, pad_x_max))\n",
    "            elif img.ndim == 3:\n",
    "                padding = ((pad_y_min, pad_y_max), (pad_x_min, pad_x_max), (0,0))\n",
    "                \n",
    "            img = np.pad(img, padding, mode='constant', constant_values=0)\n",
    "\n",
    "        if img.shape[:2] != (hpsz*2, hpsz*2):\n",
    "            print('ERROR')\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        self.curr_annotations = self.annotations[self.row_indexer]\n",
    "        \n",
    "        prnt = ''\n",
    "        prnt += f'H5-File: {self.h5_file_path}\\n'\n",
    "        prnt += f'Using {self.used_split} split\\n\\n'\n",
    "        prnt += f'Dataset Summary:\\n'\n",
    "        \n",
    "        celltypes, counts = np.unique(self.curr_annotations, return_counts=True)\n",
    "        total = counts.sum()\n",
    "        sorted_idx = np.argsort(counts)[::-1]\n",
    "\n",
    "        prnt += f\"{'Celltype':<25}{'Count':>10}{'Percent':>12}\\n\"\n",
    "        prnt += \"-\" * 47 + \"\\n\"\n",
    "\n",
    "        for idx in sorted_idx:\n",
    "            ct = celltypes[idx]\n",
    "            cnt = counts[idx]\n",
    "            pct = cnt / total * 100\n",
    "            prnt += f\"{ct:<25}{cnt:>10}{pct:>11.2f}%\\n\"\n",
    "\n",
    "        prnt += \"-\" * 47 + \"\\n\"\n",
    "        prnt += f\"{'Total':<25}{total:>10}{100:>11.2f}%\\n\"\n",
    "        \n",
    "        return prnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/dataset/NB_IMC.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Provided path to h5 file does not exist: /home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/dataset/NB_IMC.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiChannelDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh5_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mused_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mMultiChannelDataset.__init__\u001b[0;34m(self, h5_file_path, patch_size, shuffle, markers_to_use, in_memory, marker_key, pipeline, additional_keys, split, used_split, mask_image, classes_to_ignore, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Ensure the HDF5 file exists\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(h5_file_path)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided path to h5 file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh5_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarker_key \u001b[38;5;241m=\u001b[39m marker_key\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_keys \u001b[38;5;241m=\u001b[39m additional_keys\n",
      "\u001b[0;31mAssertionError\u001b[0m: Provided path to h5 file does not exist: /home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/dataset/NB_IMC.h5"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MultiChannelDataset(\n",
    "    h5_file_path=out_path,\n",
    "    patch_size=32,\n",
    "    in_memory=False,\n",
    "    shuffle=False,\n",
    "    used_split='all',\n",
    "    random_state=np.random.randint(0, 1000, 1)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.meta_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=512, num_workers=16)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/dataset/cHL_MIBI.h5\", 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['IMAGE_KEYS'][()].astype(str))\n",
    "    print(f['INDEXER'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIDL26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
